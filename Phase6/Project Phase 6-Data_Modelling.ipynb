{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# data visualization\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import Bar, Layout\n",
    "from plotly import offline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号\n",
    "\n",
    "# change text color\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# IPython\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.read_csv('final_train.csv',index_col=0)\n",
    "final_test = pd.read_csv('final_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"5\"></span>In order to get results between 0 and 1, a function, which is called **sigmoid**, is used to transform our hypothesis function. It is defined as\n",
    "$$ $$\n",
    "$$h_{\\theta}(x) = g(\\theta^{T} x)$$ \n",
    "$$ $$\n",
    "where $h_{\\theta}(x)$ is the hypothesis function, $x$ is a single record and \n",
    "$$ $$\n",
    "$$g(z)=\\dfrac{1}{1+e^{-z}}$$\n",
    "$$ $$\n",
    "By using $g(\\theta^{T} x)$, we obtain the probablity and if $h_{\\theta}(x) \\geq 0.5$, we get $y=1$; if $h_{\\theta}(x) < 0.5$, we get $y=0$. Further, when $z \\geq 0$, $g(z) \\geq 0.5$ is another detail. Thus, if the $\\theta^{T} x \\geq 0$, then $y=1$.\n",
    " \n",
    "By the definition, I defined the below ***sigmoid*** function.<span id=\"5\"></span>\n",
    "\n",
    "We can't use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. That's why we need to define a different cost function for logistic regression. It is simply defined as\n",
    "$$ $$\n",
    "$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}Cost(h_{\\theta}(x^{(i)}), y^{(i)})$$ \n",
    "$$ $$\n",
    "where \n",
    "$$ $$\n",
    "$$Cost(h_{\\theta}(x^{(i)}), y^{(i)})=-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-h_{\\theta}(x^{(i)}))$$\n",
    "$$ $$\n",
    "As the sanity check, $J(\\theta)$ can be plotted or printed as a function of the number of iterations to be sure that $J(\\theta)$ is **decreasing on every iteration**, which shows that it is converging correctly. At this point, choice of $\\alpha$ is important. If we select a high or small $\\alpha$ value, we might have problem about the converging.<span id=\"6\"></span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "# 1.Sigmoid function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "# 2. loss function \n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_estimators: The number of trees or rounds. Adding more trees will be at the risk of overfitting. The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees. Quickly, the model reaches a point of diminishing returns.\n",
    "\n",
    "- max_depth: The maximum depth of a tree. It is also used to control overfitting as higher depth will allow model to learn relations very specific to a particular sample. Typically, it should be chosen from 3 to 10 and tuned using CV.\n",
    "\n",
    "- objective: The loss function to be minimized. binary:logistic is for binary classification, which will return predicted probability (NOT CLASS).\n",
    "\n",
    "- learning_rate: The convergence control parameter in gradient descent. It is intuitive that XGB will not reach its minimum if both n_estimaters and learning_rate are very small.\n",
    "\n",
    "- subsample: The fraction of observations to be randomly chosen for each tree. Lower values make the algorithm more conservative and prevents overfitting, but too small values might lead to underfitting. So, be careful to choose and the typical values are between 0.5 and 1.\n",
    "\n",
    "- min_child_weight: The minimum sum of weights all observations required in child. It is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n",
    "\n",
    "- colsample_bytree: The fraction of features to use. By default it is set to 1 meaning that we will use all features. But in order to avoid the number of highly correlated trees is getting too big, we would like to use a sample of all the features for training to avoid overfitting.\n",
    "\n",
    "- scale_pos_weight: The parameter that controls the balance of positive and negative weights, useful for unbalanced classes. This dataset is unbalanced as we have seen, so we should be careful to tune it. The typical value to consider: sum(negative instances) / sum(positive instances).\n",
    "\n",
    "- gamma: The minimum loss reduction required to make a split. A node is split only when the resulting split gives a positive reduction in the loss function. The larger gamma is, the more conservative (overfitting) the algorithm will be. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "- reg_alpha: L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "- reg_lambda: L2 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10],\n",
    "#         'gamma': [0.5, 1, 1.5, 2, 5, 10],\n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'max_depth': [3, 4, 5]\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = XGBClassifier(learning_rate=0.06, n_estimators=300, objective='binary:logistic',nthread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# def timer(start_time=None):\n",
    "#     if not start_time:\n",
    "#         start_time = datetime.now()\n",
    "#         return start_time\n",
    "#     elif start_time:\n",
    "#         thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "#         tmin, tsec = divmod(temp_sec, 60)\n",
    "#         print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "        \n",
    "        \n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# folds = 3\n",
    "# param_comb = 5\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n",
    "\n",
    "# # Here we go\n",
    "# start_time = timer(None)\n",
    "# random_search.fit(X, y)\n",
    "# timer(start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "# print(random_search.best_score_ * 2 - 1)\n",
    "# print('\\n Best hyperparameters:')\n",
    "# print(random_search.best_params_)\n",
    "# results = pd.DataFrame(random_search.cv_results_)\n",
    "# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 调参之后，较优的参数组合\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=6,\n",
    "                        colsample_bytree=.8,\n",
    "                        scale_pos_weight=1.6,\n",
    "                        gamma=10,\n",
    "                        reg_alpha=8,\n",
    "                        reg_lambda=1.3,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_gini(df_train,tar_enc = True,pca = False):\n",
    "    \n",
    "    '''\n",
    "    df_train: 已处理的训练集数据\n",
    "    tar_enc: 是否对类别型变量使用target encoding\n",
    "    pca: 是否使用pca\n",
    "    '''    \n",
    "    \n",
    "    y = df_train.target\n",
    "    X = df_train.drop('target',axis=1)\n",
    "    \n",
    "    \n",
    "    y_valid_pred = 0*y\n",
    "    y_test_pred = 0\n",
    "    \n",
    "    \n",
    "    from target_encoding import target_encode\n",
    "    \n",
    "    train = pd.concat([X,y],axis=1)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "\n",
    "        # 分成训练集、验证集、测试集\n",
    "\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()        \n",
    "        X_test = final_test.copy()\n",
    "        \n",
    "        \n",
    "        if pca == True:\n",
    "            n_comp = 20\n",
    "            print('\\nPCA执行中...')\n",
    "            pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n",
    "            X_train = pd.DataFrame(pca.fit_transform(X_train))\n",
    "            X_valid = pd.DataFrame(pca.transform(X_valid))\n",
    "            X_test = pd.DataFrame(pca.transform(final_test.copy()))\n",
    "        print( f\"\\n{i}折交叉验证： \")\n",
    "        \n",
    "        if pca == False:\n",
    "            if tar_enc == True:\n",
    "                f_cat = [f for f in X.columns if '_cat' in f and 'tar_enc' not in  f]\n",
    "                for f in f_cat:\n",
    "                    X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                                    trn_series=X_train[f],\n",
    "                                                                    val_series=X_valid[f],\n",
    "                                                                    tst_series=X_test[f],\n",
    "                                                                    target=y_train,\n",
    "                                                                    min_samples_leaf=100,\n",
    "                                                                    smoothing=10,\n",
    "                                                                    noise_level=0\n",
    "                                                                    )\n",
    "\n",
    "    #     from category_encoders.target_encoder import TargetEncoder\n",
    "    #     tar_enc = TargetEncoder(cols = f_cat).fit(X_train,y_train)\n",
    "    #     X_train = tar_enc.transform(X_train) # 转换训练集\n",
    "    #     X_test = tar_enc.transform(X_test) # 转换测试集\n",
    "\n",
    "\n",
    "            X_train.drop(f_cat,axis=1,inplace=True)\n",
    "            X_valid.drop(f_cat,axis=1,inplace=True)\n",
    "            X_test.drop(f_cat,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "        # 对于当前折，跑XGB\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid,y_valid)]\n",
    "            fit_model = model.fit( X_train, y_train, \n",
    "                                   eval_set=eval_set,\n",
    "                                   eval_metric=gini_xgb,\n",
    "                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                                   verbose=False\n",
    "                                 )\n",
    "            print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "            print( \"  Best gini = \", model.best_score )\n",
    "        else:\n",
    "            fit_model = model.fit( X_train, y_train )\n",
    "\n",
    "        # 生成验证集的预测结果\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  normalized gini coefficent = \", eval_gini(y_valid, pred) )\n",
    "        y_valid_pred.iloc[test_index] = pred\n",
    "\n",
    "        # 累积计算测试集预测结果\n",
    "        y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        del X_test, X_train, X_valid, y_train\n",
    "\n",
    "    y_test_pred /= K  # 取各fold结果均值\n",
    "\n",
    "    print( \"\\n整个训练集（合并）的normalized gini coefficent:\" )\n",
    "    print( \"  final normalized gini coefficent = \", eval_gini(y, y_valid_pred) )\n",
    "    \n",
    "    return y_test_pred,eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0折交叉验证： \n",
      "  normalized gini coefficent =  0.2521255881298117\n",
      "\n",
      "1折交叉验证： \n",
      "  normalized gini coefficent =  0.30787820484790185\n",
      "\n",
      "2折交叉验证： \n",
      "  normalized gini coefficent =  0.2812042717533373\n",
      "\n",
      "3折交叉验证： \n",
      "  normalized gini coefficent =  0.2824787109934668\n",
      "\n",
      "4折交叉验证： \n",
      "  normalized gini coefficent =  0.2895325604792781\n",
      "\n",
      "5折交叉验证： \n",
      "  normalized gini coefficent =  0.2914708137368027\n",
      "\n",
      "6折交叉验证： \n",
      "  normalized gini coefficent =  0.2788781419900316\n",
      "\n",
      "7折交叉验证： \n",
      "  normalized gini coefficent =  0.2741946407666682\n",
      "\n",
      "8折交叉验证： \n",
      "  normalized gini coefficent =  0.27019773113149437\n",
      "\n",
      "9折交叉验证： \n",
      "  normalized gini coefficent =  0.3091194217515304\n",
      "\n",
      "整个训练集（合并）的normalized gini coefficent:\n",
      "  final normalized gini coefficent =  0.2835429751767702\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred, gini_score = XGB_gini(df_train=final_train,tar_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = final_test.index.values\n",
    "submission['target'] = y_test_pred\n",
    "submission.to_csv('xgb_submit.csv', float_format='%.6f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
